{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "engstopwords = stopwords.words('English') \n",
    "engstopwords.extend(['mg', 'kg', 'mg kg', 'hcc', 'aarc'])\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import cross_validate,LeaveOneOut, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/smruthi/Desktop/TechTogether2020/data/docs_100_share.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning up non-ascii chars from abstract\n",
    "\n",
    "newabstracts = []\n",
    "for abstract in df['abstract']:\n",
    "    test_list = (abstract.split())\n",
    "    for i in range(len(test_list)):\n",
    "        if not test_list[i].isascii():\n",
    "            test_list[i] = re.sub(r'[^\\x00-\\x7F]+',' ', test_list[i])\n",
    "    newabstracts.append(' '.join(test_list))\n",
    "    \n",
    "df['abstract'] = newabstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pmid</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>include/exclude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>27989839</td>\n",
       "      <td>Cilostazol and enzymatically modified isoquerc...</td>\n",
       "      <td>We previously reported the anti-inflammatory e...</td>\n",
       "      <td>include</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>26878064</td>\n",
       "      <td>Zinc Ionophore (Clioquinol) Inhibition of Huma...</td>\n",
       "      <td>Prostate cancer remains the second leading cau...</td>\n",
       "      <td>include</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>23999004</td>\n",
       "      <td>Antitumor activity of histamine and clozapine ...</td>\n",
       "      <td>BACKGROUND: Functional presence of histamine H...</td>\n",
       "      <td>include</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>20798593</td>\n",
       "      <td>Use of dabigatran etexilate to reduce breast c...</td>\n",
       "      <td>Coagulation proteases and the generation of th...</td>\n",
       "      <td>include</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>30685222</td>\n",
       "      <td>A study in a rat initiation-promotion bladder ...</td>\n",
       "      <td>Dapagliflozin, a sodium-glucose co-transporter...</td>\n",
       "      <td>include</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       pmid                                              title  \\\n",
       "0  27989839  Cilostazol and enzymatically modified isoquerc...   \n",
       "1  26878064  Zinc Ionophore (Clioquinol) Inhibition of Huma...   \n",
       "2  23999004  Antitumor activity of histamine and clozapine ...   \n",
       "3  20798593  Use of dabigatran etexilate to reduce breast c...   \n",
       "4  30685222  A study in a rat initiation-promotion bladder ...   \n",
       "\n",
       "                                            abstract include/exclude  \n",
       "0  We previously reported the anti-inflammatory e...         include  \n",
       "1  Prostate cancer remains the second leading cau...         include  \n",
       "2  BACKGROUND: Functional presence of histamine H...         include  \n",
       "3  Coagulation proteases and the generation of th...         include  \n",
       "4  Dapagliflozin, a sodium-glucose co-transporter...         include  "
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createFeatures(corpus):\n",
    "    \n",
    "    newcorpus = []\n",
    "    \n",
    "    for example in corpus:\n",
    "        words = nltk.word_tokenize(example)\n",
    "        words = nltk.pos_tag(words)\n",
    "        wordlist = []\n",
    "        for word in words:\n",
    "            #filtering stopwords, checking for alphabetic chars\n",
    "            if word[0].isalpha() and word[0] not in engstopwords:\n",
    "                #each feature is word+position tag\n",
    "                wordlist.append(word[0]+\"_\"+word[1])\n",
    "                \n",
    "        newcorpus.append(\" \".join(wordlist))\n",
    "    \n",
    "    #adding maximum document frequency of 5 \n",
    "    #since high frequency words in every abstract will be terms like \"patient\"\n",
    "    #these don't tell us anything about relevancy\n",
    "    vectorizer = CountVectorizer(max_df=5)\n",
    "    texts = vectorizer.fit_transform(newcorpus).toarray()\n",
    "    vocab = vectorizer.get_feature_names()\n",
    "\n",
    "    return texts,vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateModel(X,y,vocab,penalty=\"l1\"):\n",
    "    \n",
    "    #create and fit the model\n",
    "    model1 = LogisticRegression(penalty=penalty,solver=\"liblinear\")\n",
    "    results1 = cross_validate(model1,X,y,cv=LeaveOneOut())\n",
    "    \n",
    "    model2 = LinearSVC(random_state=0, tol=1e-5)\n",
    "    results2 = cross_validate(model2,X,y,cv=LeaveOneOut())\n",
    "\n",
    "  \n",
    "    #determine the average accuracy\n",
    "    scores1 = results1[\"test_score\"]\n",
    "    avg_score1 = sum(scores1)/len(scores1)\n",
    "    \n",
    "    scores2 = results2[\"test_score\"]\n",
    "    avg_score2 = sum(scores2)/len(scores2)\n",
    "\n",
    "    model1.fit(X,y)\n",
    "    class1_prob_sorted1 = model1.coef_[0, :].argsort()\n",
    "    class2_prob_sorted1 = (-model1.coef_[0, :]).argsort()\n",
    "\n",
    "    model2.fit(X,y)\n",
    "    class1_prob_sorted2 = model2.coef_[0, :].argsort()\n",
    "    class2_prob_sorted2 = (-model2.coef_[0, :]).argsort()\n",
    "    \n",
    "    termsToTake = 20\n",
    "    class1_indicators1 = [vocab[i] for i in class1_prob_sorted1[:termsToTake]]\n",
    "    class2_indicators1 = [vocab[i] for i in class1_prob_sorted1[:termsToTake]]\n",
    "    \n",
    "    class1_indicators2 = [vocab[i] for i in class2_prob_sorted2[:termsToTake]]\n",
    "    class2_indicators2 = [vocab[i] for i in class2_prob_sorted2[:termsToTake]]\n",
    "\n",
    "    return avg_score1,class1_indicators1,class2_indicators1,avg_score2,class1_indicators2,class2_indicators2\n",
    "\n",
    "def runEvaluation(X,y,vocab):\n",
    "    print(\"----------L2 Norm-----------\")\n",
    "    avg_score1,class1_indicators1,class2_indicators1,avg_score2,class1_indicators2,class2_indicators2 = evaluateModel(X,y,vocab,\"l2\")\n",
    "    print(\"Logistic\")\n",
    "    print(\"The model's average accuracy is %f\"%avg_score1)\n",
    "    print(\"The most informative terms for exclude are: %s\"%class1_indicators1)\n",
    "    print(\"The most informative terms for include are: %s\"%class1_indicators2)\n",
    "    print(\"SVM\")\n",
    "    print(\"The model's average accuracy is %f\"%avg_score2)\n",
    "    print(\"The most informative terms for exclude are: %s\"%class2_indicators1)\n",
    "    print(\"The most informative terms for include are: %s\"%class2_indicators2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = list(df['abstract'])\n",
    "corpus = [x.split(\". \")[-2:] for x in corpus]\n",
    "corpus = [x[0]+\". \"+x[1] for x in corpus]\n",
    "y = df['include/exclude']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------L2 Norm-----------\n",
      "Logistic\n",
      "The model's average accuracy is 0.730000\n",
      "The most informative terms for exclude are: ['tolerated_vbn', 'trial_nnp', 'safe_jj', 'postoperative_jj', 'morphine_nn', 'opioid_jj', 'registration_nnp', 'asp_nnp', 'regimens_nns', 'safety_nn', 'analgesia_nn', 'cancer_nnp', 'registration_nn', 'children_nns', 'management_nn', 'undergoing_vbg', 'aggressive_jj', 'neutropenia_nn', 'consumption_nn', 'improve_vb']\n",
      "The most informative terms for include are: ['demonstrated_vbd', 'metformin_nn', 'potent_jj', 'hcc_nnp', 'antitumor_nn', 'angiogenesis_nn', 'new_jj', 'promising_jj', 'docetaxel_nn', 'development_nn', 'preclinical_jj', 'inhibitory_jj', 'malignant_jj', 'synergistic_jj', 'human_jj', 'found_vbd', 'ato_nnp', 'dfo_nnp', 'induce_vb', 'beneficial_jj']\n",
      "SVM\n",
      "The model's average accuracy is 0.740000\n",
      "The most informative terms for exclude are: ['tolerated_vbn', 'trial_nnp', 'safe_jj', 'postoperative_jj', 'morphine_nn', 'opioid_jj', 'registration_nnp', 'asp_nnp', 'regimens_nns', 'safety_nn', 'analgesia_nn', 'cancer_nnp', 'registration_nn', 'children_nns', 'management_nn', 'undergoing_vbg', 'aggressive_jj', 'neutropenia_nn', 'consumption_nn', 'improve_vb']\n",
      "The most informative terms for include are: ['demonstrated_vbd', 'metformin_nn', 'potent_jj', 'hcc_nnp', 'antitumor_nn', 'angiogenesis_nn', 'new_jj', 'promising_jj', 'docetaxel_nn', 'development_nn', 'preclinical_jj', 'inhibitory_jj', 'malignant_jj', 'synergistic_jj', 'human_jj', 'found_vbd', 'ato_nnp', 'dfo_nnp', 'induce_vb', 'beneficial_jj']\n"
     ]
    }
   ],
   "source": [
    "X,vocab = createFeatures(corpus)\n",
    "runEvaluation(X, y, vocab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
